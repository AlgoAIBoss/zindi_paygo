{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d625381",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "mod_path = os.path.join(Path.cwd().parent.parent)\n",
    "if mod_path not in sys.path:\n",
    "    sys.path.append(mod_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae8f40cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "from typing import List, Tuple\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from src.data import *\n",
    "from src.features.utils import *\n",
    "from src.model.tree_based import ModelXgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3021d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data with shape 28007, 33 [transaction related features]\n",
    "train = pd.read_csv('../../data/processed/train.csv')\n",
    "test_set = pd.read_csv('../../data/processed/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c94793ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Deposit', 'AccessoryRate', 'rateTypeEntity', 'RatePerUnit',\n",
       "       'DaysOnDeposit', 'MainApplicantGender', 'Age', 'Region', 'Occupation',\n",
       "       'Term', 'TotalContractValue', 'ExpectedTermDate', 'FirstPaymentDate',\n",
       "       'LastPaymentDate', 'SplitPaymentsHistory', 'nb_payments', 'amount_paid',\n",
       "       'percent_amt_paid', 'mean_amt_paid', 'median_amt_paid', 'max_amt_paid',\n",
       "       'min_amt_paid', 'stddev_amt_paid', 'b1', 'b2', 'b3', 'b4', 'b5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.drop(columns = 'Unnamed: 0', inplace=True)\n",
    "test_set.drop(columns = 'Unnamed: 0', inplace=True)\n",
    "# test_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7db563b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28007, 35)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc843b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.162994965544328\n"
     ]
    }
   ],
   "source": [
    "# Region has certain NaN values which might cause issues while encoding\n",
    "# As total NaNs constitute ~5% of the data (1446) we remove it as of now\n",
    "print(train['Region'].isna().sum() / train.shape[0] * 100)\n",
    "train.dropna(subset=['Region'], how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf80fefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Region has certain NaN values which might cause issues while encoding\n",
    "# As total NaNs constitute ~5% of the data (1446) we remove it as of now\n",
    "print(test_set['Region'].isna().sum() / test_set.shape[0] * 100)\n",
    "# train.dropna(subset=['Region'], how='all', inplace=True)\n",
    "\n",
    "# When attempting drop=first in OHE, the reverse transform throws an issue as it reads the NaN values as a separate\n",
    "# category. So converting NaNs into strings\n",
    "train['Region'] = train['Region'].fillna('Null')\n",
    "test_set['Region'] = test_set['Region'].fillna('Null')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78335542",
   "metadata": {},
   "source": [
    "## Approach 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eb6541",
   "metadata": {},
   "source": [
    "s = pd.DataFrame(np.arange(0, len(train)), columns=['m1'])\n",
    "df = train[['b1', 'b2', 'b3', 'b4', 'b5']]\n",
    "df.drop(columns=['b5'], inplace=True)\n",
    "df.rename(columns={'b1': 'b2', 'b2':'b3', 'b3': 'b4', 'b4': 'b5'}, inplace=True)\n",
    "df.insert(loc=0, column='b1', value=s.values)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654d2978",
   "metadata": {},
   "source": [
    "t = pd.DataFrame([[1]], columns=['a'])\n",
    "q = pd.DataFrame([[1]], columns=['b'])\n",
    "r = pd.DataFrame([['aum']], columns=['name'])\n",
    "\n",
    "k = pd.concat([t, q])\n",
    "kk = pd.merge(k, r, how='left', left_index=True, right_index=True)\n",
    "kk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4bf2927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_variable_window(\n",
    "    predictor_array: pd.DataFrame,\n",
    "    var_to_add: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    predictor_array.drop(columns=['b5'], inplace=True)  # We drop the first payment\n",
    "    predictor_array.rename(columns={'b1': 'b2', 'b2':'b3', 'b3': 'b4', 'b4': 'b5'}, inplace=True)\n",
    "    predictor_array.insert(loc=0, column='b1', value=var_to_add.values)  # And add the new variable (mn)\n",
    "    \n",
    "    return predictor_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ff494f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_payment_history_df = train[[\"ID\", \"SplitPaymentsHistory\"]]\n",
    "# id_arr = train[[\"ID\"]]\n",
    "\n",
    "target = train[['m1', 'm2', 'm3', 'm4', 'm5', 'm6']]\n",
    "train_arr = train.drop(columns=['m1', 'm2', 'm3', 'm4', 'm5', 'm6', \n",
    "                                'SplitPaymentsHistory',\n",
    "                                'ExpectedTermDate', \n",
    "                                'FirstPaymentDate',\n",
    "                                'LastPaymentDate'])\n",
    "test_set.drop(columns=['SplitPaymentsHistory',\n",
    "                       'ExpectedTermDate', \n",
    "                       'FirstPaymentDate',\n",
    "                       'LastPaymentDate'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71d59c53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_data_with_sliding_approach(data_without_target: pd.DataFrame,\n",
    "                                      target_data: pd.DataFrame):\n",
    "    frame = pd.DataFrame(None)\n",
    "    new_df = deepcopy(data_without_target)\n",
    "    target_df = pd.DataFrame(None)\n",
    "    target_features = target_data.columns.tolist()\n",
    "    for itr, col in enumerate(target_features):\n",
    "        if itr == 0:\n",
    "            target_df = pd.concat([target_df, target_data[[col]]])\n",
    "            frame = pd.concat([frame, data_without_target])\n",
    "        else:\n",
    "            filter_df = new_df[['b1', 'b2', 'b3', 'b4', 'b5']]  # Intermediate df\n",
    "            new_df.drop(columns=['b1', 'b2', 'b3', 'b4', 'b5'], inplace=True)\n",
    "            concatinating_df = slide_variable_window(predictor_array=filter_df, \n",
    "                                                     var_to_add=target_data[[target_features[itr-1]]])\n",
    "            new_df = pd.concat([new_df, concatinating_df], axis=1)  # We add the newly created columns\n",
    "            target_df = pd.concat([target_df, target_data[[col]]])\n",
    "            frame = pd.concat([frame, new_df])\n",
    "#             print(new_df.shape)\n",
    "\n",
    "    target_df = pd.DataFrame(target_df.sum(axis=1).astype(int), columns=['target'])\n",
    "#     print(frame.shape)  # Should be 6 * original data's no. of rows\n",
    "    \n",
    "    frame.reset_index(drop=True, inplace=True)\n",
    "    target_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return frame, target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60bcfe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 10\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_arr, target, test_size=0.45, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7de9742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a59df4",
   "metadata": {},
   "source": [
    "### Model train on initial hp :: Approach 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8474caf",
   "metadata": {},
   "source": [
    "def approach_two_model(x_train, y_train, x_test):\n",
    "    model = ModelXgBoost(train_array=x_train, train_target=y_train)\n",
    "    model.train_model()  # Default h.params (Checkout the code)\n",
    "    predict = model.trained_model.predict(x_test)\n",
    "    \n",
    "    return model, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8356a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_drop(full_array, data_type, tr_encoder=None):\n",
    "#     print(tr_encoder)\n",
    "    categorical_array = full_array[full_array.select_dtypes(exclude=['number']).columns]\n",
    "    numerical_array = full_array.drop(columns=full_array.select_dtypes(exclude=['number']).columns)\n",
    "#     print(categorical_array.columns)\n",
    "    encoded_array, encoder = one_hot_encoding(\n",
    "        categorical_frame=categorical_array, \n",
    "        type_of_data=data_type,\n",
    "        fitted_encoder=tr_encoder,\n",
    "        conv=True,\n",
    "        drop=None,\n",
    "        handle_unknown=\"ignore\"\n",
    "    )\n",
    "#     print(encoded_array.shape)\n",
    "    final_array = pd.concat([numerical_array.reset_index(drop=True), \n",
    "                             encoded_array.reset_index(drop=True)], axis=1)\n",
    "    final_array.index = numerical_array.index\n",
    "#     print(final_array.shape)\n",
    "    return numerical_array, final_array, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b650e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_features(\n",
    "    features,\n",
    "    feature_scores,\n",
    "    cut_off_score=0.8\n",
    ") -> Tuple[List[List], List[List]]:\n",
    "    frame = pd.DataFrame([feature_scores], columns=features, index=['gain']).T\n",
    "    frame.sort_values(by=['gain'], ascending=False, inplace=True)\n",
    "    frame['cum_gain'] = frame['gain'].cumsum()\n",
    "    \n",
    "    feature_list = list(frame[frame['cum_gain'] <= cut_off_score].index)\n",
    "    feature_scores_list = frame[frame['cum_gain'] <= cut_off_score]['gain'].tolist()\n",
    "    \n",
    "    return feature_list, feature_scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e75f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_important_feature_scores(\n",
    "    feature_corpus: List[List],\n",
    "    score_corpus: List[List]\n",
    ") -> pd.DataFrame:\n",
    "    imp_column = dict()\n",
    "    for feature_list, score_list in zip(feature_corpus, score_corpus):\n",
    "        for feature_names, scores in zip(feature_list, score_list):\n",
    "            if feature_names not in imp_column.keys():\n",
    "                imp_column[feature_names] = 1\n",
    "            else:\n",
    "                imp_column[feature_names] += 1\n",
    "    \n",
    "    important_features_df = pd.DataFrame(imp_column, index=['frequency']).T\n",
    "    important_features_df['score'] = important_features_df['frequency'] / important_features_df['frequency'].sum()\n",
    "    important_features_df.sort_values(by=['score'], ascending=False, inplace=True)\n",
    "    important_features_df['cum_score'] = important_features_df['score'].cumsum()\n",
    "    \n",
    "    return important_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36da0c52",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Preparing the TRAIN data for approach two and fitting the model\n",
    "train_data, target_frame = create_data_with_sliding_approach(data_without_target=X_train, \n",
    "                                                             target_data=y_train)\n",
    "id_array = train_data[[\"ID\"]]\n",
    "train_data.drop(columns=[\"ID\"], inplace=True)\n",
    "og_frame, encoded_train, encoder_model = encode_and_drop(train_data, \"train\", None)\n",
    "\n",
    "# encoded_train = pd.concat([encoded_train, train_data[['b1', 'b2', 'b3', 'b4', 'b5']]], axis=1)\n",
    "print(encoded_train.shape)\n",
    "\n",
    "np.random.seed(0)\n",
    "r = np.random.randint(1, 100, 100)\n",
    "f_i_list = []\n",
    "f_i_features = []\n",
    "for i in r:\n",
    "    sample= encoded_train.sample(n=20000, replace=False, random_state=i)\n",
    "    target_f = target_frame[target_frame.index.isin(sample.index)]\n",
    "    model_two_obj = ModelXgBoost(train_array=sample, \n",
    "                                 train_target=target_f)\n",
    "    model_two_obj.train_model()  # Default h.params (Checkout the code)\n",
    "    model_two = model_two_obj.trained_model\n",
    "    f_i = model_two.feature_importances_\n",
    "    f_i_cols = sample.columns\n",
    "    variables, variable_scores = get_top_features(features=f_i_cols, feature_scores=f_i, cut_off_score=0.8)\n",
    "    f_i_list.append(variable_scores)\n",
    "    f_i_features.append(variables)\n",
    "\n",
    "important_features_df = get_important_feature_scores(feature_corpus=f_i_features, score_corpus=f_i_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad28f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.figure(figsize=(15,10))\n",
    "pyplot.bar(important_features_df.index, important_features_df['cum_score'])\n",
    "# xgb.plot_importance(model_two)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e28624f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_selection = list(important_features_df[important_features_df['cum_score'] <= 0.8].index)\n",
    "feature_selection = ['b1',\n",
    " 'Age',\n",
    " 'amount_paid',\n",
    " 'Deposit',\n",
    " 'mean_amt_paid',\n",
    " 'stddev_amt_paid',\n",
    " 'b2',\n",
    " 'percent_amt_paid',\n",
    " 'nb_payments',\n",
    " 'min_amt_paid',\n",
    " 'b3',\n",
    " 'DaysOnDeposit',\n",
    " 'b4',\n",
    " 'max_amt_paid',\n",
    " 'median_amt_paid',\n",
    " 'Driver/Motorbike Rider',\n",
    " 'b5',\n",
    " 'Mount Kenya Region',\n",
    " 'Term',\n",
    " 'Teacher',\n",
    " 'Western',\n",
    " 'North Rift']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8def823e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aumaron/.local/share/virtualenvs/zindi_payg-FXkRANRI/lib/python3.9/site-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n",
      "/home/aumaron/.local/share/virtualenvs/zindi_payg-FXkRANRI/lib/python3.9/site-packages/pandas/core/frame.py:4441: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(\n"
     ]
    }
   ],
   "source": [
    "# Preparing the TRAIN data for approach two and fitting the model\n",
    "train_data, target_frame = create_data_with_sliding_approach(data_without_target=X_train, \n",
    "                                                             target_data=y_train)\n",
    "id_array = train_data[[\"ID\"]]\n",
    "train_data.drop(columns=[\"ID\"], inplace=True)\n",
    "og_frame, encoded_train, encoder_model = encode_and_drop(train_data, \"train\", None)\n",
    "encoded_train = encoded_train[feature_selection]\n",
    "# encoded_train = pd.concat([encoded_train, train_data[['b1', 'b2', 'b3', 'b4', 'b5']]], axis=1)\n",
    "# print(encoded_train.shape)\n",
    "\n",
    "model_two_obj = ModelXgBoost(train_array=encoded_train,\n",
    "                             train_target=target_frame)\n",
    "model_two_obj.train_model()  # Default h.params (Checkout the code)\n",
    "model_two = model_two_obj.trained_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e206dc",
   "metadata": {},
   "source": [
    "### Prediction using Model :: Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fcd8f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aumaron/.local/share/virtualenvs/zindi_payg-FXkRANRI/lib/python3.9/site-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "test_id_array = X_test[[\"ID\"]]\n",
    "X_test.drop(columns=[\"ID\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67ef9e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aumaron/.local/share/virtualenvs/zindi_payg-FXkRANRI/lib/python3.9/site-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n",
      "/home/aumaron/.local/share/virtualenvs/zindi_payg-FXkRANRI/lib/python3.9/site-packages/pandas/core/frame.py:4441: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'m1_pred': array([ 166.53697, 1734.822  ,  446.4041 , ...,  203.634  , 2080.8218 ,\n",
       "        1527.7251 ], dtype=float32),\n",
       " 'm2_pred': array([10743.731, 12649.362, 11953.632, ..., 11838.278, 12655.505,\n",
       "        12567.73 ], dtype=float32),\n",
       " 'm3_pred': array([10743.731, 12649.362, 11953.632, ..., 11828.587, 12655.505,\n",
       "        12567.73 ], dtype=float32),\n",
       " 'm4_pred': array([10648.264, 12649.362, 12028.519, ..., 12089.109, 12655.505,\n",
       "        12567.73 ], dtype=float32),\n",
       " 'm5_pred': array([10648.264, 12649.362, 12028.519, ..., 12089.109, 12655.505,\n",
       "        12567.73 ], dtype=float32),\n",
       " 'm6_pred': array([10648.264, 12649.362, 12028.519, ..., 12089.109, 12655.505,\n",
       "        12567.73 ], dtype=float32)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding and re-attaching using train encoding model\n",
    "og_frame_test, encoded_test, encoder_model = encode_and_drop(X_test, \"test\", encoder_model)\n",
    "# encoded_test = pd.concat([encoded_test, X_test[['b1', 'b2', 'b3', 'b4', 'b5']].reset_index(drop=True)], axis=1)\n",
    "\n",
    "predict_dict = dict()\n",
    "encoded_test = encoded_test[feature_selection]\n",
    "for col_no, predict_col in enumerate(y_test.columns):\n",
    "    predict_dict[f\"m{col_no+1}_pred\"] = model_two.predict(encoded_test)\n",
    "    int_df = encoded_test[['b1', 'b2', 'b3', 'b4', 'b5']]\n",
    "    encoded_test.drop(columns=['b1', 'b2', 'b3', 'b4', 'b5'], inplace=True)\n",
    "    concatinating_df = slide_variable_window(predictor_array=int_df, \n",
    "                                             var_to_add=pd.DataFrame(predict_dict[f\"m{col_no+1}_pred\"]))\n",
    "    encoded_test = pd.concat([encoded_test, concatinating_df], axis=1)  # We add the newly created columns\n",
    "    \n",
    "predict_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ce746",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbec36b",
   "metadata": {},
   "source": [
    "### Calculation of RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2092440",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_frame = pd.DataFrame(predict_dict)\n",
    "pred_frame.index = X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a4ee76",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_array = pd.concat([X_test, y_test, pred_frame], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508f40cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_array = pd.merge(full_test_array, test_id_array, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3e622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file = SubmissionFile(\n",
    "    validation_data=full_test_array,\n",
    "    type_of_data='validation'\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccb8dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff35a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file['SquaredError'] = np.square(sub_file['Target'] - sub_file['Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f381e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.sum(sub_file['SquaredError'])/sub_file.shape[0])\n",
    "print('Final RMSE --> ', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e79fc1",
   "metadata": {},
   "source": [
    "### Preparing Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1526ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the Entire TRAIN data for approach two and fitting the model\n",
    "train_data, target_frame = create_data_with_sliding_approach(data_without_target=train_arr, \n",
    "                                                             target_data=target)\n",
    "print(train_data.columns)\n",
    "id_array = train_data[[\"ID\"]]\n",
    "train_data.drop(columns=[\"ID\"], inplace=True)\n",
    "og_frame, encoded_train, encoder_model = encode_and_drop(train_data, \"train\", None)\n",
    "print(encoded_train.columns)\n",
    "encoded_train = encoded_train[feature_selection]\n",
    "print(encoded_train.columns)\n",
    "# encoded_train = pd.concat([encoded_train, train_data[['b1', 'b2', 'b3', 'b4', 'b5']]], axis=1)\n",
    "\n",
    "model_two_obj = ModelXgBoost(train_array=encoded_train, \n",
    "                             train_target=target_frame)\n",
    "model_two_obj.train_model()  # Default h.params (Checkout the code)\n",
    "model_two = model_two_obj.trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ca2502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_id = test_set[[\"ID\"]]\n",
    "# test_set.drop(columns=[\"ID\"], inplace=True)\n",
    "\n",
    "# Encoding and re-attaching using train encoding model\n",
    "og_frame_test, encoded_test, encoder_model = encode_and_drop(test_set, \"test\", encoder_model)\n",
    "encoded_test = encoded_test[feature_selection]\n",
    "# encoded_test = pd.concat([encoded_test, test_set[['b1', 'b2', 'b3', 'b4', 'b5']].reset_index(drop=True)], axis=1)\n",
    "\n",
    "predict_dict = dict()\n",
    "\n",
    "for col_no, predict_col in enumerate(['m1', 'm2', 'm3', 'm4', 'm5', 'm6']):\n",
    "    predict_dict[f\"m{col_no+1}_pred\"] = model_two.predict(encoded_test)\n",
    "    int_df = encoded_test[['b1', 'b2', 'b3', 'b4', 'b5']]\n",
    "    encoded_test.drop(columns=['b1', 'b2', 'b3', 'b4', 'b5'], inplace=True)\n",
    "    concatinating_df = slide_variable_window(predictor_array=int_df, \n",
    "                                             var_to_add=pd.DataFrame(predict_dict[f\"m{col_no+1}_pred\"]))\n",
    "    encoded_test = pd.concat([encoded_test, concatinating_df], axis=1)  # We add the newly created columns\n",
    "    \n",
    "predict_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d39b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_frame_test = pd.DataFrame(predict_dict)\n",
    "pred_frame_test.index = test_set.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a49d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_array_test = pd.concat([test_set, pred_frame_test], axis=1)\n",
    "full_test_array_test = pd.merge(full_test_array_test, test_id, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26174048",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file = SubmissionFile(\n",
    "    validation_data=full_test_array_test,\n",
    "    type_of_data='test'\n",
    ").execute()\n",
    "sub_file.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434474df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file.to_csv('../../submissions/submission_approach_2_feature_selection_80.csv', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d481870",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9a2192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd09870c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zindi_payg",
   "language": "python",
   "name": "zindi_payg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
