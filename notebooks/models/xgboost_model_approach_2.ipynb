{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e287100a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "mod_path = os.path.join(Path.cwd().parent.parent)\n",
    "if mod_path not in sys.path:\n",
    "    sys.path.append(mod_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01a94498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "from typing import List, Tuple\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from src.features import *\n",
    "from src.features.utils import *\n",
    "from src.model.tree_based import ModelXgBoost\n",
    "from src.data.validation_file import SubmissionFile\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f9c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data with shape 28007, 33 [transaction related features]\n",
    "train = pd.read_csv('../../data/processed/train.csv')\n",
    "test_set = pd.read_csv('../../data/processed/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9319a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data with shape 28007, 33 [transaction related features]\n",
    "train = pd.read_csv('../../data/processed/train2.csv')\n",
    "test_set = pd.read_csv('../../data/processed/test2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16836cac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.drop(columns = 'Unnamed: 0', inplace=True)\n",
    "test_set.drop(columns = 'Unnamed: 0', inplace=True)\n",
    "# test_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13fc517c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28007, 35)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81406052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.162994965544328\n"
     ]
    }
   ],
   "source": [
    "# Region has certain NaN values which might cause issues while encoding\n",
    "# As total NaNs constitute ~5% of the data (1446) we remove it as of now\n",
    "print(train['Region'].isna().sum() / train.shape[0] * 100)\n",
    "train.dropna(subset=['Region'], how='all', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56292a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Region has certain NaN values which might cause issues while encoding\n",
    "# As total NaNs constitute ~5% of the data (1446) we remove it as of now\n",
    "print(test_set['Region'].isna().sum() / test_set.shape[0] * 100)\n",
    "# train.dropna(subset=['Region'], how='all', inplace=True)\n",
    "\n",
    "# When attempting drop=first in OHE, the reverse transform throws an issue as it reads the NaN values as a separate\n",
    "# category. So converting NaNs into strings\n",
    "train['Region'] = train['Region'].fillna('Null')\n",
    "test_set['Region'] = test_set['Region'].fillna('Null')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857e67d2",
   "metadata": {},
   "source": [
    "## Approach 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edee400",
   "metadata": {},
   "source": [
    "s = pd.DataFrame(np.arange(0, len(train)), columns=['m1'])\n",
    "df = train[['b1', 'b2', 'b3', 'b4', 'b5']]\n",
    "df.drop(columns=['b5'], inplace=True)\n",
    "df.rename(columns={'b1': 'b2', 'b2':'b3', 'b3': 'b4', 'b4': 'b5'}, inplace=True)\n",
    "df.insert(loc=0, column='b1', value=s.values)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6033fd46",
   "metadata": {},
   "source": [
    "t = pd.DataFrame([[1]], columns=['a'])\n",
    "q = pd.DataFrame([[1]], columns=['b'])\n",
    "r = pd.DataFrame([['aum']], columns=['name'])\n",
    "\n",
    "k = pd.concat([t, q])\n",
    "kk = pd.merge(k, r, how='left', left_index=True, right_index=True)\n",
    "kk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c50dc4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_variable_window(\n",
    "    predictor_array: pd.DataFrame,\n",
    "    var_to_add: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    predictor_array.drop(columns=['b5'], inplace=True)  # We drop the first payment\n",
    "    predictor_array.rename(columns={'b1': 'b2', 'b2':'b3', 'b3': 'b4', 'b4': 'b5'}, inplace=True)\n",
    "    predictor_array.insert(loc=0, column='b1', value=var_to_add.values)  # And add the new variable (mn)\n",
    "    \n",
    "    return predictor_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19dd2298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_payment_history_df = train[[\"ID\", \"SplitPaymentsHistory\"]]\n",
    "# id_arr = train[[\"ID\"]]\n",
    "\n",
    "target = train[['m1', 'm2', 'm3', 'm4', 'm5', 'm6']]\n",
    "train_arr = train.drop(columns=['m1', 'm2', 'm3', 'm4', 'm5', 'm6', \n",
    "                                'SplitPaymentsHistory',\n",
    "                                'ExpectedTermDate', \n",
    "                                'FirstPaymentDate',\n",
    "                                'LastPaymentDate'])\n",
    "test_set.drop(columns=['SplitPaymentsHistory',\n",
    "                       'ExpectedTermDate', \n",
    "                       'FirstPaymentDate',\n",
    "                       'LastPaymentDate'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28e0da02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_data_with_sliding_approach(data_without_target: pd.DataFrame,\n",
    "                                      target_data: pd.DataFrame):\n",
    "    frame = pd.DataFrame(None)\n",
    "    new_df = deepcopy(data_without_target)\n",
    "    target_df = pd.DataFrame(None)\n",
    "    target_features = target_data.columns.tolist()\n",
    "    for itr, col in enumerate(target_features):\n",
    "        if itr == 0:\n",
    "            target_df = pd.concat([target_df, target_data[[col]]])\n",
    "            frame = pd.concat([frame, data_without_target])\n",
    "        else:\n",
    "            filter_df = new_df[['b1', 'b2', 'b3', 'b4', 'b5']]  # Intermediate df\n",
    "            new_df.drop(columns=['b1', 'b2', 'b3', 'b4', 'b5'], inplace=True)\n",
    "            concatinating_df = slide_variable_window(predictor_array=filter_df, \n",
    "                                                     var_to_add=target_data[[target_features[itr-1]]])\n",
    "            new_df = pd.concat([new_df, concatinating_df], axis=1)  # We add the newly created columns\n",
    "            target_df = pd.concat([target_df, target_data[[col]]])\n",
    "            frame = pd.concat([frame, new_df])\n",
    "#             print(new_df.shape)\n",
    "\n",
    "    target_df = pd.DataFrame(target_df.sum(axis=1).astype(int), columns=['target'])\n",
    "#     print(frame.shape)  # Should be 6 * original data's no. of rows\n",
    "    \n",
    "    frame.reset_index(drop=True, inplace=True)\n",
    "    target_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return frame, target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d11409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 10\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_arr, target, test_size=0.45, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3baf49",
   "metadata": {},
   "source": [
    "### Model train on initial hp :: Approach 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9280051c",
   "metadata": {},
   "source": [
    "def approach_two_model(x_train, y_train, x_test):\n",
    "    model = ModelXgBoost(train_array=x_train, train_target=y_train)\n",
    "    model.train_model()  # Default h.params (Checkout the code)\n",
    "    predict = model.trained_model.predict(x_test)\n",
    "    \n",
    "    return model, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8dd3395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_drop(full_array, data_type, tr_encoder=None):\n",
    "#     print(tr_encoder)\n",
    "    categorical_array = full_array[full_array.select_dtypes(exclude=['number']).columns]\n",
    "    numerical_array = full_array.drop(columns=full_array.select_dtypes(exclude=['number']).columns)\n",
    "#     print(categorical_array.columns)\n",
    "    if not tr_encoder:\n",
    "        encoder = FeatureEncoding()\n",
    "    else:\n",
    "        encoder = tr_encoder\n",
    "    encoded_array = encoder.one_hot_encoding(\n",
    "        categorical_frame=categorical_array, \n",
    "        type_of_data=data_type,\n",
    "        conv=True,\n",
    "        drop=None,\n",
    "        handle_unknown=\"ignore\"\n",
    "    )\n",
    "#     print(encoded_array.columns)\n",
    "    final_array = pd.concat([numerical_array.reset_index(drop=True), \n",
    "                             encoded_array.reset_index(drop=True)], axis=1)\n",
    "    final_array.index = numerical_array.index\n",
    "#     print(final_array.shape)\n",
    "    return numerical_array, encoded_array, final_array, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75469cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_features(\n",
    "    features,\n",
    "    feature_scores,\n",
    "    cut_off_score=0.8\n",
    ") -> Tuple[List[List], List[List]]:\n",
    "    frame = pd.DataFrame([feature_scores], columns=features, index=['gain']).T\n",
    "    frame.sort_values(by=['gain'], ascending=False, inplace=True)\n",
    "    frame['cum_gain'] = frame['gain'].cumsum()\n",
    "    \n",
    "    feature_list = list(frame[frame['cum_gain'] <= cut_off_score].index)\n",
    "    feature_scores_list = frame[frame['cum_gain'] <= cut_off_score]['gain'].tolist()\n",
    "    \n",
    "    return feature_list, feature_scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782125e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_important_feature_scores(\n",
    "    feature_corpus: List[List],\n",
    "    score_corpus: List[List]\n",
    ") -> pd.DataFrame:\n",
    "    imp_column = dict()\n",
    "    imp_col = dict()\n",
    "    for feature_list, score_list in zip(feature_corpus, score_corpus):\n",
    "        for feature_names, scores in zip(feature_list, score_list):\n",
    "            if feature_names not in imp_column.keys():\n",
    "                imp_column[feature_names] = 1\n",
    "#                 imp_col[feature_names] = [scores]\n",
    "            else:\n",
    "                imp_column[feature_names] += 1\n",
    "#                 imp_col[feature_names].append(scores)\n",
    "    \n",
    "    important_features_df = pd.DataFrame(imp_column, index=['frequency']).T\n",
    "#     important_scores = pd.DataFrame(imp_col)\n",
    "    important_features_df['appearance_ratio'] = important_features_df['frequency'] / 100\n",
    "    important_features_df.sort_values(by=['frequency'], ascending=False, inplace=True)\n",
    "#     important_features_df['cum_score'] = important_features_df['score'].cumsum()\n",
    "    \n",
    "    return important_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b49dff3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preparing the TRAIN data for approach two and fitting the model\n",
    "train_data, target_frame = create_data_with_sliding_approach(data_without_target=train_arr, \n",
    "                                                             target_data=target)\n",
    "id_array = train_data[[\"ID\"]]\n",
    "train_data.drop(columns=[\"ID\"], inplace=True)\n",
    "num_frame, encoded_train, full_frame, encoder_model = encode_and_drop(train_data, \"train\", None)\n",
    "\n",
    "# encoded_train = pd.concat([encoded_train, train_data[['b1', 'b2', 'b3', 'b4', 'b5']]], axis=1)\n",
    "print('Columns ->', encoded_train.columns)\n",
    "print('Shape ->', encoded_train.shape)\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "r = np.random.randint(1, 100, 1)\n",
    "f_i_list = []\n",
    "f_i_features = []\n",
    "for i, _seed in enumerate(r):\n",
    "#     sample= full_frame.sample(n=20000, replace=False, random_state=_seed)\n",
    "#     target_f = target_frame[target_frame.index.isin(sample.index)]\n",
    "    sample= full_frame\n",
    "    target_f = target_frame\n",
    "    model_two_obj = ModelXgBoost(train_array=sample, \n",
    "                                 train_target=target_f)\n",
    "    model_two_obj.train_model()  # Default h.params (Checkout the code)\n",
    "    model_two = model_two_obj.trained_model\n",
    "    f_i = model_two.feature_importances_\n",
    "    f_i_cols = sample.columns\n",
    "    variables, variable_scores = get_top_features(features=f_i_cols, feature_scores=f_i, cut_off_score=0.8)\n",
    "    f_i_list.append(variable_scores)\n",
    "    f_i_features.append(variables)\n",
    "\n",
    "important_features_df = get_important_feature_scores(feature_corpus=f_i_features, score_corpus=f_i_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976613ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855f2d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587d46c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frame = pd.DataFrame([variable_scores], columns=variables, index=['Gain']).T\n",
    "frame.sort_values(by=['Gain'], ascending=False, inplace=True)\n",
    "frame['cumulative_gain'] = frame['Gain'].cumsum()\n",
    "frame\n",
    "# frame.to_csv('cumulative_gain_og_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5384d7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdec727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# important_features_df.to_csv('../../submissions/important_features_total_gain_with_80_cutoff.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54441f21",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.figure(figsize=(15,10))\n",
    "pyplot.bar(frame.index, frame['cumulative_gain'])\n",
    "# xgb.plot_importance(model_two)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70fe4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When train has Null regions removed\n",
    "# feature_selection = list(important_features_df[important_features_df['appearance_ratio'] >=0.3].index)\n",
    "feature_selection = ['North Rift',\n",
    " 'Other',\n",
    " 'Term',\n",
    " 'Driver/Motorbike Rider',\n",
    " 'Labourer',\n",
    " 'South Rift',\n",
    " 'TotalContractValue',\n",
    " 'Nyanza',\n",
    " 'Nairobi Region',\n",
    " 'Western',\n",
    " 'Teacher',\n",
    " 'Mount Kenya Region',\n",
    " 'Government Employee',\n",
    " 'Farmer',\n",
    " 'Male',\n",
    " 'AccessoryRate',\n",
    " 'RatePerUnit',\n",
    " 'WEEKLY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe84f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When train has Null regions as a category\n",
    "# feature_selection = list(important_features_df[important_features_df['appearance_ratio'] <=0.5].index)\n",
    "feature_selection = ['b1', 'Age', 'b4', 'DaysOnDeposit', 'b2', 'Deposit', 'b3', 'North Rift',\n",
    "       'b5', 'South Rift', 'Term', 'Other', 'Driver/Motorbike Rider',\n",
    "       'Coast Region', 'Nairobi Region', 'Western', 'Labourer', 'Teacher', 'Mount Kenya Region', 'Farmer',\n",
    "       'Nyanza', 'Government Employee', 'Business']\n",
    "# feature_selection = ['Driver/Motorbike Rider',\n",
    "# #  'Coast Region',\n",
    "#  'Nairobi Region',\n",
    "#  'Western',\n",
    "#  'Labourer',\n",
    "#  'TotalContractValue',\n",
    "#  'Teacher',\n",
    "#  'Mount Kenya Region',\n",
    "#  'Farmer',\n",
    "#  'Nyanza',\n",
    "#  'Government Employee',\n",
    "# #  'Female',\n",
    "# #  'Business',\n",
    "#  'RatePerUnit',\n",
    "#  'AccessoryRate',\n",
    "#  'WEEKLY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d558b08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DAILY', 'MONTHLY', 'WEEKLY', 'Female', 'Male', 'Coast Region',\n",
       "       'Mount Kenya Region', 'Nairobi Region', 'North Rift', 'Nyanza',\n",
       "       'South Rift', 'Western', 'Business', 'Driver/Motorbike Rider', 'Farmer',\n",
       "       'Government Employee', 'Labourer', 'Other', 'Teacher', 'b1', 'b2', 'b3',\n",
       "       'b4', 'b5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a9db458",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aumaron/.local/share/virtualenvs/zindi_payg-FXkRANRI/lib/python3.9/site-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n",
      "/home/aumaron/.local/share/virtualenvs/zindi_payg-FXkRANRI/lib/python3.9/site-packages/pandas/core/frame.py:4441: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87648, 19)\n",
      "(87648, 24)\n",
      "PARAMS-> {'objective': 'reg:squarederror', 'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'gamma': None, 'gpu_id': None, 'importance_type': 'gain', 'interaction_constraints': None, 'learning_rate': None, 'max_delta_step': None, 'max_depth': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 100, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 0, 'reg_alpha': 50, 'reg_lambda': 0, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': 0, 'use_label_encoder': False}\n"
     ]
    }
   ],
   "source": [
    "# Preparing the TRAIN data for approach two and fitting the model\n",
    "train_data, target_frame = create_data_with_sliding_approach(data_without_target=X_train, \n",
    "                                                             target_data=y_train)\n",
    "id_array = train_data[[\"ID\"]]\n",
    "train_data.drop(columns=[\"ID\"], inplace=True)\n",
    "num_frame, encoded_train, full_train, encoder_model = encode_and_drop(train_data, \"train\", None)\n",
    "# encoded_train = encoded_train[feature_selection]\n",
    "print(encoded_train.shape)\n",
    "encoded_train = pd.concat([encoded_train, train_data[['b1', 'b2', 'b3', 'b4', 'b5']]], axis=1)\n",
    "print(encoded_train.shape)\n",
    "\n",
    "model_two_obj = ModelXgBoost(train_array=encoded_train,\n",
    "                             train_target=target_frame)\n",
    "model_two_obj.train_model()  # Default h.params (Checkout the code)\n",
    "model_two = model_two_obj.trained_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4553dd",
   "metadata": {},
   "source": [
    "### Prediction using Model :: Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac0270f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aumaron/.local/share/virtualenvs/zindi_payg-FXkRANRI/lib/python3.9/site-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "test_id_array = X_test[[\"ID\"]]\n",
    "X_test.drop(columns=[\"ID\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3353cf3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([19204, 15721,  7879, 25250, 19371,  2608,  2121, 20788, 20931,\n",
       "            21136,\n",
       "            ...\n",
       "            26884,  7697, 19076, 24308,  9246, 22564, 17551, 14648,  3550,\n",
       "            13387],\n",
       "           dtype='int64', length=11953)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d6e4984",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11953, 24)\n",
      "[ 298.98547 1370.1808   336.08096 ...  229.55864 1385.3893  1319.7733 ]\n",
      "[ 365.25754 1408.1921   404.01724 ...  223.63448 1303.6301  1298.9755 ]\n",
      "[ 321.3584  1430.3431   447.55276 ...  223.63448 1314.6316  1222.1951 ]\n",
      "[ 379.00632 1279.6415   432.97665 ...  209.94781 1338.343   1241.9469 ]\n",
      "[ 359.7216  1228.2202   387.7058  ...  263.27692 1250.7705  1142.567  ]\n",
      "[ 414.9437  1168.9843   365.17502 ...  247.06192 1127.6016  1129.6669 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'m1_pred': array([ 298.98547, 1370.1808 ,  336.08096, ...,  229.55864, 1385.3893 ,\n",
       "        1319.7733 ], dtype=float32),\n",
       " 'm2_pred': array([ 365.25754, 1408.1921 ,  404.01724, ...,  223.63448, 1303.6301 ,\n",
       "        1298.9755 ], dtype=float32),\n",
       " 'm3_pred': array([ 321.3584 , 1430.3431 ,  447.55276, ...,  223.63448, 1314.6316 ,\n",
       "        1222.1951 ], dtype=float32),\n",
       " 'm4_pred': array([ 379.00632, 1279.6415 ,  432.97665, ...,  209.94781, 1338.343  ,\n",
       "        1241.9469 ], dtype=float32),\n",
       " 'm5_pred': array([ 359.7216 , 1228.2202 ,  387.7058 , ...,  263.27692, 1250.7705 ,\n",
       "        1142.567  ], dtype=float32),\n",
       " 'm6_pred': array([ 414.9437 , 1168.9843 ,  365.17502, ...,  247.06192, 1127.6016 ,\n",
       "        1129.6669 ], dtype=float32)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding and re-attaching using train encoding model\n",
    "num_frame_test, encoded_test, full_test, encoder_model = encode_and_drop(X_test, \"test\", encoder_model)\n",
    "# encoded_test = encoded_test[feature_selection]\n",
    "_inexes = X_test.index\n",
    "encoded_test = pd.concat([encoded_test.reset_index(drop=True),\n",
    "                          X_test[['b1', 'b2', 'b3', 'b4', 'b5']].reset_index(drop=True)], axis=1)\n",
    "encoded_test.index = _inexes\n",
    "print(encoded_test.shape)\n",
    "predict_dict = dict()\n",
    "for col_no, predict_col in enumerate(y_test.columns):\n",
    "#     print(encoded_test)\n",
    "    predict_dict[f\"{predict_col}_pred\"] = model_two.predict(encoded_test)\n",
    "    print(predict_dict[f\"{predict_col}_pred\"])\n",
    "    int_df = encoded_test[['b1', 'b2', 'b3', 'b4', 'b5']]\n",
    "    encoded_test.drop(columns=['b1', 'b2', 'b3', 'b4', 'b5'], inplace=True)\n",
    "    concatinating_df = slide_variable_window(predictor_array=deepcopy(int_df), \n",
    "                                             var_to_add=pd.DataFrame(deepcopy(predict_dict[f\"{predict_col}_pred\"])))\n",
    "    encoded_test = pd.concat([encoded_test, concatinating_df], axis=1)  # We add the newly created columns\n",
    "#     print(encoded_test[['b1', 'b2', 'b3', 'b4', 'b5']])\n",
    "    \n",
    "predict_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d4bf087",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-392.03134\n",
      "-336.96024\n",
      "-256.6005\n",
      "-116.88847\n",
      "-7.944236\n",
      "-269.73184\n",
      "-673.13513\n",
      "-15.122854\n",
      "-172.21228\n",
      "-324.7183\n",
      "-4.881765\n",
      "-853.0861\n",
      "-60.498684\n",
      "-20.132063\n",
      "-196.47621\n",
      "-492.92197\n",
      "-115.79759\n",
      "-10.721448\n",
      "-11.710172\n",
      "-11.236606\n",
      "-8.821763\n"
     ]
    }
   ],
   "source": [
    "for k, v in predict_dict.items():\n",
    "#     predict_dict[k] = [0 if i < 0 else i for i in v]\n",
    "    for _v in v:\n",
    "        if _v < 0:\n",
    "            print(_v)\n",
    "# predict_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d29e90",
   "metadata": {},
   "source": [
    "### Calculation of RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e5cab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_frame = pd.DataFrame(predict_dict)\n",
    "pred_frame.index = X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35812c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_array = pd.concat([X_test, y_test, pred_frame], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0fc0c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_array = pd.merge(full_test_array, test_id_array, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db76f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file = SubmissionFile(\n",
    "    validation_data=full_test_array,\n",
    "    type_of_data='validation'\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a210bdd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71718, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08332a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file['SquaredError'] = np.square(sub_file['Target'] - sub_file['Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8183e593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE -->  836.3801053455517\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(np.sum(sub_file['SquaredError'])/sub_file.shape[0])\n",
    "print('Final RMSE --> ', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae1c55",
   "metadata": {},
   "source": [
    "### Preparing Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b2764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the Entire TRAIN data for approach two and fitting the model\n",
    "train_data, target_frame = create_data_with_sliding_approach(data_without_target=train_arr, \n",
    "                                                             target_data=target)\n",
    "print(train_data.columns)\n",
    "id_array = train_data[[\"ID\"]]\n",
    "train_data.drop(columns=[\"ID\"], inplace=True)\n",
    "og_frame, encoded_train, encoder_model = encode_and_drop(train_data, \"train\", None)\n",
    "print(encoded_train.columns)\n",
    "encoded_train = encoded_train[feature_selection]\n",
    "print(encoded_train.columns)\n",
    "encoded_train = pd.concat([encoded_train, train_data[['b1', 'b2', 'b3', 'b4', 'b5']]], axis=1)\n",
    "\n",
    "model_two_obj = ModelXgBoost(train_array=encoded_train, \n",
    "                             train_target=target_frame)\n",
    "model_two_obj.train_model()  # Default h.params (Checkout the code)\n",
    "model_two = model_two_obj.trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68613a39",
   "metadata": {},
   "source": [
    "# Strategy to impute missing regions | did not work\n",
    "most_common_region = test_set['Region'].value_counts().index[0]\n",
    "test_set.loc[test_set['Region'].isna(), 'Region'] = most_common_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b0e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test_set[[\"ID\"]]\n",
    "test_set.drop(columns=[\"ID\"], inplace=True)\n",
    "\n",
    "# Encoding and re-attaching using train encoding model\n",
    "og_frame_test, encoded_test, encoder_model = encode_and_drop(test_set, \"test\", encoder_model)\n",
    "encoded_test = encoded_test[feature_selection]\n",
    "encoded_test = pd.concat([encoded_test, test_set[['b1', 'b2', 'b3', 'b4', 'b5']].reset_index(drop=True)], axis=1)\n",
    "\n",
    "predict_dict = dict()\n",
    "\n",
    "for col_no, predict_col in enumerate(['m1', 'm2', 'm3', 'm4', 'm5', 'm6']):\n",
    "    predict_dict[f\"m{col_no+1}_pred\"] = model_two.predict(encoded_test)\n",
    "    int_df = encoded_test[['b1', 'b2', 'b3', 'b4', 'b5']]\n",
    "    encoded_test.drop(columns=['b1', 'b2', 'b3', 'b4', 'b5'], inplace=True)\n",
    "    concatinating_df = slide_variable_window(predictor_array=int_df, \n",
    "                                             var_to_add=pd.DataFrame(predict_dict[f\"m{col_no+1}_pred\"]))\n",
    "    encoded_test = pd.concat([encoded_test, concatinating_df], axis=1)  # We add the newly created columns\n",
    "    \n",
    "# predict_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f31518",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ke, va in predict_dict.items():\n",
    "    predict_dict[ke] = [0 if i < 0 else i for i in va]\n",
    "    for _va in va:\n",
    "        if _va < 0:\n",
    "            print(_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5699391",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_frame_test = pd.DataFrame(predict_dict)\n",
    "pred_frame_test.index = test_set.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1c44cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_array_test = pd.concat([test_set, pred_frame_test], axis=1)\n",
    "full_test_array_test = pd.merge(full_test_array_test, test_id, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e316c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file = SubmissionFile(\n",
    "    validation_data=full_test_array_test,\n",
    "    type_of_data='test'\n",
    ").execute()\n",
    "sub_file.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6344cc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file.to_csv('../../submissions/submission_approach_2_bottom_feature_selection_null_region_category_neg_predictions_removed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5a2ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e601f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8876b79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zindi_payg",
   "language": "python",
   "name": "zindi_payg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
