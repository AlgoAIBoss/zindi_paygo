{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5bcdcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "mod_path = os.path.join(Path.cwd().parent.parent)\n",
    "if mod_path not in sys.path:\n",
    "    sys.path.append(mod_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50797cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas numpy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "567114ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# !pip install sklearn\n",
    "import xgboost as xgb\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "from src.features.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36ba6865",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aumaron/.local/share/virtualenvs/zindi_payg-FXkRANRI/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3169: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../../data/Train.csv\", sep=\",\")\n",
    "metadata = pd.read_csv(\"../../data/metadata.csv\", sep=\",\")\n",
    "test = pd.read_csv(\"../../data/Test.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc843f27",
   "metadata": {},
   "source": [
    "### Datasets Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3f2cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged = metadata.merge(train, how=\"right\", on=\"ID\")\n",
    "# test_merged = metadata.merge(test, how='right', on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "380c91e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28007, 38)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faa920f",
   "metadata": {},
   "source": [
    "### Payment Related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3befcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_construction(df):\n",
    "    try:\n",
    "        df[\"SplitPaymentsHistory\"] = df.apply(lambda row: split(row[\"PaymentsHistory\"]), axis=1)\n",
    "        df[\"SplitTransactionDates\"] = df.apply(lambda row: split(row[\"TransactionDates\"], type_of_value='date'), axis=1)\n",
    "        df[\"nb_payments\"] = df.apply(lambda row: length_calc(row[\"SplitPaymentsHistory\"]), axis=1)\n",
    "        df[\"amount_paid\"] = df.apply(lambda row: sum_calc(row[\"SplitPaymentsHistory\"]), axis=1)\n",
    "        df[\"percent_amt_paid\"] = df[\"amount_paid\"] / train_merged[\"TotalContractValue\"]\n",
    "        df[\"mean_amt_paid\"] = df.apply(lambda row: mean_calc(row[\"SplitPaymentsHistory\"]), axis=1)\n",
    "        df[\"median_amt_paid\"] = df.apply(lambda row: median_calc(row[\"SplitPaymentsHistory\"]), axis=1)\n",
    "        df[\"max_amt_paid\"] = df.apply(lambda row: max_calc(row[\"SplitPaymentsHistory\"]), axis=1)\n",
    "        df[\"min_amt_paid\"] = df.apply(lambda row: min_calc(row[\"SplitPaymentsHistory\"]), axis=1)\n",
    "        df[\"stddev_amt_paid\"] = df.apply(lambda row: std_dev_calc(row[\"SplitPaymentsHistory\"]), axis=1)\n",
    "    except KeyError as e:\n",
    "        raise Exception(\"Column Missing\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "train_merged = feature_construction(train_merged)\n",
    "# test_merged = feature_construction(test_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7937354a",
   "metadata": {},
   "source": [
    "### Number of Missed Payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08cbbfbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fd757d861a86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnb_missing_payments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransaction_dates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_payment_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_payment_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpayment_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransaction_dates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpayment_dates_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_payment_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_payment_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"M\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from utils import convert\n",
    "\n",
    "def nb_missing_payments(transaction_dates, first_payment_date, last_payment_date):\n",
    "    payment_dates = set([convert(x) for x in transaction_dates])\n",
    "    payment_dates_range = set([str(x) for x in pd.period_range(first_payment_date, last_payment_date, freq=\"M\")])\n",
    "    \n",
    "    return len(payment_dates_range - payment_dates) - 6\n",
    "\n",
    "train_merged[\"nb_skipped_months\"] = train_merged.apply(\n",
    "    lambda row: nb_missing_payments(\n",
    "        row[\"SplitTransactionDates\"], row.FirstPaymentDate, row.LastPaymentDate\n",
    "    ), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8368b3",
   "metadata": {},
   "source": [
    "### Bar Plots for each Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f56536a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateEDA:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_min_n_max(df):\n",
    "        min_date = min([convert(x) for x in df.SplitTransactionDates[0]])\n",
    "        max_date = max([convert(x) for x in df.SplitTransactionDates[0]])\n",
    "        for i in range(train_merged.shape[0]):\n",
    "            temp_min = min([convert(x) for x in df.iloc[i][\"SplitTransactionDates\"]])\n",
    "            temp_max = max([convert(x) for x in df.iloc[i][\"SplitTransactionDates\"]])\n",
    "            \n",
    "            if temp_min < min_date:\n",
    "                min_date = temp_min\n",
    "            if temp_max > max_date:\n",
    "                max_date = temp_max\n",
    "        \n",
    "        return min_date, max_date\n",
    "\n",
    "    @staticmethod\n",
    "    def find_stats(df, min_date, max_date):\n",
    "        \n",
    "        all_dates = [str(x) for x in pd.period_range(datetime.strptime(min_date, \"%Y-%m\"), datetime.strptime(max_date, \"%Y-%m\"), freq=\"M\")]\n",
    "        date_payments_dict = Counter({date: 0 for date in all_dates})\n",
    "        date_count_dict = Counter({date: 0 for date in all_dates})\n",
    "        for i in range(train_merged.shape[0]):\n",
    "            # Calculates the payments made in each month\n",
    "            payments_dict = {\n",
    "                convert(date): float(payment)\n",
    "                for date, payment in zip(\n",
    "                    df.iloc[i][\"SplitTransactionDates\"],\n",
    "                    df.iloc[i][\"SplitPaymentsHistory\"]\n",
    "                )\n",
    "            }\n",
    "\n",
    "            # Calculates the number of payments made in each month\n",
    "            count_dict = {\n",
    "                convert(date): 1 for date in df.iloc[i][\"SplitTransactionDates\"]\n",
    "            }\n",
    "\n",
    "            date_payments_dict = Counter(payments_dict) + date_payments_dict\n",
    "            date_count_dict = Counter(count_dict) + date_count_dict\n",
    "\n",
    "        return dict(date_payments_dict), dict(date_count_dict)\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_avg_payments(dict1, dict2):\n",
    "        return {k: round(dict1[k]/dict2[k], 2) for k in dict1}\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot(target_dict, title=\"\"):\n",
    "        target_dict = collections.OrderedDict(sorted(target_dict.items()))\n",
    "        figure(figsize=(15, 12), dpi=100)\n",
    "        names = list(target_dict.keys())\n",
    "        values = list(target_dict.values())\n",
    "        plt.ticklabel_format(style='plain')\n",
    "        plt.title(title)\n",
    "        plt.bar(range(len(target_dict)), values, tick_label=names)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "\n",
    "    def execute(self):\n",
    "        min_date, max_date = self.find_min_n_max(self.df)\n",
    "        payments_dict, count_dict = self.find_stats(self.df, min_date, max_date)\n",
    "        avg_payments_dict = self.calculate_avg_payments(payments_dict, count_dict)\n",
    "        \n",
    "        self.plot(payments_dict, \"Total Payments\")\n",
    "        self.plot(count_dict, \"Number of Payments\")\n",
    "        self.plot(avg_payments_dict, \"Average Payment\")\n",
    "\n",
    "# DateEDA(train_merged).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354d1083",
   "metadata": {},
   "source": [
    "### Back Payment Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a47c2f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Back Payment Features\n",
    "\n",
    "\n",
    "def add_back_feature(df, n=1):\n",
    "    df[f\"b{n}\"] = df.apply(lambda row: back_feature(row[\"SplitPaymentsHistory\"], n), axis=1)\n",
    "    return df\n",
    "\n",
    "train_merged = add_back_feature(train_merged, 1)\n",
    "train_merged = add_back_feature(train_merged, 2)\n",
    "train_merged = add_back_feature(train_merged, 3)\n",
    "train_merged = add_back_feature(train_merged, 4)\n",
    "train_merged = add_back_feature(train_merged, 5)\n",
    "\n",
    "# test_merged = add_back_feature(test_merged, 1)\n",
    "# test_merged = add_back_feature(test_merged, 2)\n",
    "# test_merged = add_back_feature(test_merged, 3)\n",
    "# test_merged = add_back_feature(test_merged, 4)\n",
    "# test_merged = add_back_feature(test_merged, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d714cd7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9336, 37)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b99c33",
   "metadata": {},
   "source": [
    "### Date Related Features\n",
    "\n",
    "TODO: \n",
    "1. Skipped payments and duration of skipping payments\n",
    "2. Check for custom seasonality (Weather changes EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac33807a",
   "metadata": {},
   "source": [
    "### Backward Feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5015704",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged[\"RegistrationDateParsed\"] = pd.to_datetime(train_merged[\"RegistrationDate\"], infer_datetime_format=True)\n",
    "train_merged[\"ExpectedTermDateParsed\"] = pd.to_datetime(train_merged[\"ExpectedTermDate\"], infer_datetime_format=True)\n",
    "train_merged[\"FirstPaymentDateParsed\"] = pd.to_datetime(train_merged[\"FirstPaymentDate\"], infer_datetime_format=True)\n",
    "train_merged[\"LastPaymentDateParsed\"] = pd.to_datetime(train_merged[\"LastPaymentDate\"], infer_datetime_format=True)\n",
    "\n",
    "train_merged[\"RegistrationDate\"] = pd.to_datetime(train_merged[\"RegistrationDate\"], infer_datetime_format=True).dt.date\n",
    "train_merged[\"ExpectedTermDate\"] = pd.to_datetime(train_merged[\"ExpectedTermDate\"], infer_datetime_format=True).dt.date\n",
    "train_merged[\"FirstPaymentDate\"] = pd.to_datetime(train_merged[\"FirstPaymentDate\"], infer_datetime_format=True).dt.date\n",
    "train_merged[\"LastPaymentDate\"] = pd.to_datetime(train_merged[\"LastPaymentDate\"], infer_datetime_format=True).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c386ce1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_merged[\"LastFirstDuration\"] = (train_merged.LastPaymentDate - train_merged.FirstPaymentDate).astype(\"timedelta64[M]\")\n",
    "train_merged[\"ExpectedFirstDuration\"] = (train_merged.ExpectedTermDate - train_merged.FirstPaymentDate).astype(\"timedelta64[M]\")\n",
    "train_merged[\"LastRegistrationDuration\"] = (train_merged.LastPaymentDate - train_merged.RegistrationDate).astype(\"timedelta64[M]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139edcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Months\n",
    "train_merged[\"FirstPaymentMonth\"] = train_merged[\"FirstPaymentDateParsed\"].dt.month\n",
    "train_merged[\"LastPaymentMonth\"] = train_merged[\"LastPaymentDateParsed\"].dt.month\n",
    "train_merged[\"RegistrationMonth\"] = train_merged[\"RegistrationDateParsed\"].dt.month\n",
    "\n",
    "# # Sine\n",
    "# train_merged[\"FirstPaymentMonthSin\"] = np.sin((train_merged.FirstPaymentMonth-1)*(2.*np.pi/12))\n",
    "# train_merged[\"LastPaymentMonthSin\"] = np.sin((train_merged.LastPaymentMonth-1)*(2.*np.pi/12))\n",
    "# train_merged[\"RegistrationMonthSin\"] = np.sin((train_merged.RegistrationMonth-1)*(2.*np.pi/12))\n",
    "\n",
    "# # Cos\n",
    "# train_merged[\"FirstPaymentMonthCos\"] = np.cos((train_merged.FirstPaymentMonth-1)*(2.*np.pi/12))\n",
    "# train_merged[\"LastPaymentMonthCos\"] = np.cos((train_merged.LastPaymentMonth-1)*(2.*np.pi/12))\n",
    "# train_merged[\"RegistrationMonthCos\"] = np.cos((train_merged.RegistrationMonth-1)*(2.*np.pi/12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e516789a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_merged.drop([\"RegistrationDateParsed\", \"ExpectedTermDateParsed\", \"FirstPaymentDateParsed\", \"LastPaymentDateParsed\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee60a466",
   "metadata": {},
   "source": [
    "### We can ignore UpSell since it represents only 2.7% of the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57809d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged.UpsellDate.count() / train_merged.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8194cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged.nb_payments.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e8073d",
   "metadata": {},
   "source": [
    "### Drop irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "789ccc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cols(df):\n",
    "    df.drop(\n",
    "        [\n",
    "#             \"ID\",\n",
    "            \"UpsellDate\",\n",
    "            \"PaymentMethod\",\n",
    "            \"TransactionDates\",\n",
    "            \"PaymentsHistory\",\n",
    "            \"SupplierName\",\n",
    "            \"Town\"\n",
    "        ],\n",
    "        inplace=True,\n",
    "        axis=1\n",
    "    )\n",
    "drop_cols(train_merged)\n",
    "# drop_cols(test_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53cdf99b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9336, 31)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05884d0",
   "metadata": {},
   "source": [
    "### Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400d685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE\n",
    "\n",
    "def encode_and_bind(df, feature_name):\n",
    "    dummies = pd.get_dummies(df[[feature_name]])\n",
    "    res = pd.concat([df, dummies], axis=1)\n",
    "    res = res.drop([feature_name], axis=1)\n",
    "    return(res)\n",
    "\n",
    "for feature in [\"MainApplicantGender\", \"Region\", \"Occupation\", \"rateTypeEntity\"]:\n",
    "    train_merged = encode_and_bind(train_merged, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00ceb28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9059e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged.drop([\"RegistrationDate\", \"ExpectedTermDate\", \"FirstPaymentDate\", \"LastPaymentDate\", \"SplitPaymentsHistory\", \"SplitTransactionDates\", \"m2\", \"m3\", \"m4\", \"m5\", \"m6\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb66d10",
   "metadata": {},
   "source": [
    "\n",
    "### Data Leakage here. Will be handled later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d91d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_merged.Age.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1afae1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_merged.Age = train_merged.Age.fillna(train_merged.Age.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2464ae72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(0)\n",
    "def predict_model(train_array, target):\n",
    "    model = xgb.XGBRegressor(use_label_encoder=False,\n",
    "                             verbosity=0,\n",
    "                             objective=\"reg:squarederror\",\n",
    "                             reg_lambda=0,\n",
    "                             reg_alpha=50)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_array, target, test_size=0.20)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    feature_importance = model.feature_importances_\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = pow(mse, 0.5)\n",
    "    \n",
    "    print(\"---------TEST---------\")\n",
    "    print(\"Predictions\")\n",
    "    print(y_pred)\n",
    "    print(\"RMSE:\", rmse)\n",
    "    \n",
    "    y_pred = model.predict(X_train)\n",
    "    feature_importance = model.feature_importances_\n",
    "    mape = mean_absolute_percentage_error(y_train, y_pred)\n",
    "    mse = mean_squared_error(y_train, y_pred)\n",
    "    rmse = pow(mse, 0.5)\n",
    "    \n",
    "    print(\"---------TRAIN--------\")\n",
    "    print(y_pred)\n",
    "    print(\"RMSE:\", rmse)\n",
    "    \n",
    "    return y_pred, mape, mse, feature_importance, rmse\n",
    "\n",
    "predict_model(train_merged.drop([\"m1\"], axis=1), train_merged[\"m1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8199585",
   "metadata": {},
   "source": [
    "### Saving the data to data/train.csv\n",
    "\n",
    "Features created till cell 6 (and cell 9) are selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1520f50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged.drop(columns=['SplitTransactionDates', \n",
    "                           'RegistrationDate', ], inplace=True)\n",
    "# test_merged.drop(columns=['SplitTransactionDates', \n",
    "#                            'RegistrationDate', ], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1997dc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28007, 35)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72406602",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged.to_csv('../../data/processed/train.csv')\n",
    "test_merged.to_csv('../../data/processed/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eef50d",
   "metadata": {},
   "source": [
    "### Outlier Handling\n",
    "### Advance Modelling\n",
    "### EDA for adding time features\n",
    "### Scaling & Normalization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zindi_payg",
   "language": "python",
   "name": "zindi_payg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
