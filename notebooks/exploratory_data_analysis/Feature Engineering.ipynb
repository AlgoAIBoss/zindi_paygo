{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5bcdcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "mod_path = os.path.join(Path.cwd().parent.parent)\n",
    "if mod_path not in sys.path:\n",
    "    sys.path.append(mod_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50797cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas numpy\n",
    "import pandas as pd\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "567114ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# !pip install sklearn\n",
    "import xgboost as xgb\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36ba6865",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikhil/.local/share/virtualenvs/zindi_paygo-1-T8XSNTNt/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3169: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../../data/raw/Train.csv\", sep=\",\")\n",
    "metadata = pd.read_csv(\"../../data/raw/metadata.csv\", sep=\",\")\n",
    "test = pd.read_csv(\"../../data/raw/Test.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc843f27",
   "metadata": {},
   "source": [
    "### Datasets Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3f2cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged = metadata.merge(train, how=\"right\", on=\"ID\")\n",
    "# test_merged = metadata.merge(test, how='right', on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "380c91e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "           ID     RegistrationDate  Deposit UpsellDate  AccessoryRate  \\\n0  ID_MR53LEX  2018-04-23 12:28:05     2500        NaN            0.0   \n1  ID_3D7NQUH  2018-04-17 10:27:35     2500        NaN            0.0   \n2  ID_0IWQNPI  2020-02-18 14:23:44     2400        NaN            0.0   \n3  ID_IY8SYB9  2017-09-14 11:07:40     2000        NaN            0.0   \n4  ID_9XHL7VZ  2017-09-06 06:50:20     2000        NaN            0.0   \n\n  PaymentMethod rateTypeEntity  RatePerUnit  DaysOnDeposit  \\\n0      FINANCED          DAILY           55              3   \n1      FINANCED          DAILY           55              3   \n2      FINANCED          DAILY           50              3   \n3      FINANCED          DAILY           40              7   \n4      FINANCED          DAILY           40              7   \n\n  MainApplicantGender  ...     FirstPaymentDate      LastPaymentDate  \\\n0                Male  ...  2018-04-23 12:28:05  2021-05-26 17:14:45   \n1                Male  ...  2018-04-17 10:27:35  2021-05-26 16:54:53   \n2                Male  ...  2020-02-18 14:23:48  2021-01-25 18:46:20   \n3              Female  ...  2017-09-14 11:07:32  2018-12-20 16:50:39   \n4                Male  ...  2017-09-06 06:50:16  2021-05-12 14:50:01   \n\n                                    TransactionDates  \\\n0  ['04-2018', '05-2018', '06-2018', '07-2018', '...   \n1  ['04-2018', '05-2018', '06-2018', '07-2018', '...   \n2  ['02-2020', '03-2020', '04-2020', '05-2020', '...   \n3  ['09-2017', '10-2017', '11-2017', '12-2017', '...   \n4  ['09-2017', '10-2017', '11-2017', '12-2017', '...   \n\n                                     PaymentsHistory     m1      m2      m3  \\\n0  [3600.0, 750.0, 350.0, 65.0, 95.0, 135.0, 85.0...  880.0   930.0   495.0   \n1  [2940.0, 970.0, 380.0, 880.0, 385.0, 440.0, 11...  660.0   935.0   935.0   \n2      [2850.0, 1500.0, 1350.0, 610.0, 200.0, 250.0]  700.0  1350.0  1550.0   \n3  [2200.0, 1420.0, 1180.0, 900.0, 1400.0, 780.0,...  580.0   480.0   800.0   \n4  [2640.0, 910.0, 480.0, 280.0, 200.0, 180.0, 33...   40.0   440.0   460.0   \n\n       m4      m5      m6  \n0   715.0   220.0   385.0  \n1   825.0   770.0   935.0  \n2  1400.0  1450.0  1200.0  \n3  1260.0  1650.0   530.0  \n4   360.0    80.0   330.0  \n\n[5 rows x 28 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>RegistrationDate</th>\n      <th>Deposit</th>\n      <th>UpsellDate</th>\n      <th>AccessoryRate</th>\n      <th>PaymentMethod</th>\n      <th>rateTypeEntity</th>\n      <th>RatePerUnit</th>\n      <th>DaysOnDeposit</th>\n      <th>MainApplicantGender</th>\n      <th>...</th>\n      <th>FirstPaymentDate</th>\n      <th>LastPaymentDate</th>\n      <th>TransactionDates</th>\n      <th>PaymentsHistory</th>\n      <th>m1</th>\n      <th>m2</th>\n      <th>m3</th>\n      <th>m4</th>\n      <th>m5</th>\n      <th>m6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID_MR53LEX</td>\n      <td>2018-04-23 12:28:05</td>\n      <td>2500</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>FINANCED</td>\n      <td>DAILY</td>\n      <td>55</td>\n      <td>3</td>\n      <td>Male</td>\n      <td>...</td>\n      <td>2018-04-23 12:28:05</td>\n      <td>2021-05-26 17:14:45</td>\n      <td>['04-2018', '05-2018', '06-2018', '07-2018', '...</td>\n      <td>[3600.0, 750.0, 350.0, 65.0, 95.0, 135.0, 85.0...</td>\n      <td>880.0</td>\n      <td>930.0</td>\n      <td>495.0</td>\n      <td>715.0</td>\n      <td>220.0</td>\n      <td>385.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID_3D7NQUH</td>\n      <td>2018-04-17 10:27:35</td>\n      <td>2500</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>FINANCED</td>\n      <td>DAILY</td>\n      <td>55</td>\n      <td>3</td>\n      <td>Male</td>\n      <td>...</td>\n      <td>2018-04-17 10:27:35</td>\n      <td>2021-05-26 16:54:53</td>\n      <td>['04-2018', '05-2018', '06-2018', '07-2018', '...</td>\n      <td>[2940.0, 970.0, 380.0, 880.0, 385.0, 440.0, 11...</td>\n      <td>660.0</td>\n      <td>935.0</td>\n      <td>935.0</td>\n      <td>825.0</td>\n      <td>770.0</td>\n      <td>935.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID_0IWQNPI</td>\n      <td>2020-02-18 14:23:44</td>\n      <td>2400</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>FINANCED</td>\n      <td>DAILY</td>\n      <td>50</td>\n      <td>3</td>\n      <td>Male</td>\n      <td>...</td>\n      <td>2020-02-18 14:23:48</td>\n      <td>2021-01-25 18:46:20</td>\n      <td>['02-2020', '03-2020', '04-2020', '05-2020', '...</td>\n      <td>[2850.0, 1500.0, 1350.0, 610.0, 200.0, 250.0]</td>\n      <td>700.0</td>\n      <td>1350.0</td>\n      <td>1550.0</td>\n      <td>1400.0</td>\n      <td>1450.0</td>\n      <td>1200.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID_IY8SYB9</td>\n      <td>2017-09-14 11:07:40</td>\n      <td>2000</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>FINANCED</td>\n      <td>DAILY</td>\n      <td>40</td>\n      <td>7</td>\n      <td>Female</td>\n      <td>...</td>\n      <td>2017-09-14 11:07:32</td>\n      <td>2018-12-20 16:50:39</td>\n      <td>['09-2017', '10-2017', '11-2017', '12-2017', '...</td>\n      <td>[2200.0, 1420.0, 1180.0, 900.0, 1400.0, 780.0,...</td>\n      <td>580.0</td>\n      <td>480.0</td>\n      <td>800.0</td>\n      <td>1260.0</td>\n      <td>1650.0</td>\n      <td>530.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID_9XHL7VZ</td>\n      <td>2017-09-06 06:50:20</td>\n      <td>2000</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>FINANCED</td>\n      <td>DAILY</td>\n      <td>40</td>\n      <td>7</td>\n      <td>Male</td>\n      <td>...</td>\n      <td>2017-09-06 06:50:16</td>\n      <td>2021-05-12 14:50:01</td>\n      <td>['09-2017', '10-2017', '11-2017', '12-2017', '...</td>\n      <td>[2640.0, 910.0, 480.0, 280.0, 200.0, 180.0, 33...</td>\n      <td>40.0</td>\n      <td>440.0</td>\n      <td>460.0</td>\n      <td>360.0</td>\n      <td>80.0</td>\n      <td>330.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 28 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faa920f",
   "metadata": {},
   "source": [
    "### Payment Related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3befcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_construction(df):\n",
    "    try:\n",
    "        df[\"SplitPaymentsHistory\"] = df.apply(lambda row: split(row[\"PaymentsHistory\"]), axis=1)\n",
    "        df[\"SplitTransactionDates\"] = df.apply(lambda row: split(row[\"TransactionDates\"], type_of_value='date'), axis=1)\n",
    "        df[\"nb_payments\"] = df.apply(lambda row: length_calc(row[\"SplitPaymentsHistory\"]), axis=1)\n",
    "        df[\"amount_paid\"] = df.apply(lambda row: sum_calc(row[\"SplitPaymentsHistory\"]), axis=1)\n",
    "        df[\"percent_amt_paid\"] = df[\"amount_paid\"] / train_merged[\"TotalContractValue\"]\n",
    "        df[\"mean_amt_paid\"] = df.apply(lambda row: mean_calc(row[\"SplitPaymentsHistory\"]), axis=1)\n",
    "        df[\"median_amt_paid\"] = df.apply(lambda row: median_calc(row[\"SplitPaymentsHistory\"]), axis=1)\n",
    "        df[\"max_amt_paid\"] = df.apply(lambda row: max_calc(row[\"SplitPaymentsHistory\"]), axis=1)\n",
    "        df[\"min_amt_paid\"] = df.apply(lambda row: min_calc(row[\"SplitPaymentsHistory\"]), axis=1)\n",
    "        df[\"stddev_amt_paid\"] = df.apply(lambda row: std_dev_calc(row[\"SplitPaymentsHistory\"]), axis=1)\n",
    "    except KeyError as e:\n",
    "        raise Exception(\"Column Missing\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "train_merged = feature_construction(train_merged)\n",
    "# test_merged = feature_construction(test_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7937354a",
   "metadata": {},
   "source": [
    "### Number of Missed Payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08cbbfbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-fd757d861a86>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mutils\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mconvert\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mnb_missing_payments\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtransaction_dates\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfirst_payment_date\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlast_payment_date\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0mpayment_dates\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mconvert\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtransaction_dates\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mpayment_dates_range\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperiod_range\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfirst_payment_date\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlast_payment_date\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfreq\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"M\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from src.utils import convert\n",
    "\n",
    "def nb_missing_payments(transaction_dates, first_payment_date, last_payment_date):\n",
    "    payment_dates = set([convert(x) for x in transaction_dates])\n",
    "    payment_dates_range = set([str(x) for x in pd.period_range(first_payment_date, last_payment_date, freq=\"M\")])\n",
    "    \n",
    "    return len(payment_dates_range - payment_dates) - 6\n",
    "\n",
    "train_merged[\"nb_skipped_months\"] = train_merged.apply(\n",
    "    lambda row: nb_missing_payments(\n",
    "        row[\"SplitTransactionDates\"], row.FirstPaymentDate, row.LastPaymentDate\n",
    "    ), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8368b3",
   "metadata": {},
   "source": [
    "### Bar Plots for each Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f56536a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateEDA:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_min_n_max(df):\n",
    "        min_date = min([convert(x) for x in df.SplitTransactionDates[0]])\n",
    "        max_date = max([convert(x) for x in df.SplitTransactionDates[0]])\n",
    "        for i in range(train_merged.shape[0]):\n",
    "            temp_min = min([convert(x) for x in df.iloc[i][\"SplitTransactionDates\"]])\n",
    "            temp_max = max([convert(x) for x in df.iloc[i][\"SplitTransactionDates\"]])\n",
    "            \n",
    "            if temp_min < min_date:\n",
    "                min_date = temp_min\n",
    "            if temp_max > max_date:\n",
    "                max_date = temp_max\n",
    "        \n",
    "        return min_date, max_date\n",
    "\n",
    "    @staticmethod\n",
    "    def find_stats(df, min_date, max_date):\n",
    "        \n",
    "        all_dates = [str(x) for x in pd.period_range(datetime.strptime(min_date, \"%Y-%m\"), datetime.strptime(max_date, \"%Y-%m\"), freq=\"M\")]\n",
    "        date_payments_dict = Counter({date: 0 for date in all_dates})\n",
    "        date_count_dict = Counter({date: 0 for date in all_dates})\n",
    "        for i in range(train_merged.shape[0]):\n",
    "            # Calculates the payments made in each month\n",
    "            payments_dict = {\n",
    "                convert(date): float(payment)\n",
    "                for date, payment in zip(\n",
    "                    df.iloc[i][\"SplitTransactionDates\"],\n",
    "                    df.iloc[i][\"SplitPaymentsHistory\"]\n",
    "                )\n",
    "            }\n",
    "\n",
    "            # Calculates the number of payments made in each month\n",
    "            count_dict = {\n",
    "                convert(date): 1 for date in df.iloc[i][\"SplitTransactionDates\"]\n",
    "            }\n",
    "\n",
    "            date_payments_dict = Counter(payments_dict) + date_payments_dict\n",
    "            date_count_dict = Counter(count_dict) + date_count_dict\n",
    "\n",
    "        return dict(date_payments_dict), dict(date_count_dict)\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_avg_payments(dict1, dict2):\n",
    "        return {k: round(dict1[k]/dict2[k], 2) for k in dict1}\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot(target_dict, title=\"\"):\n",
    "        target_dict = collections.OrderedDict(sorted(target_dict.items()))\n",
    "        figure(figsize=(15, 12), dpi=100)\n",
    "        names = list(target_dict.keys())\n",
    "        values = list(target_dict.values())\n",
    "        plt.ticklabel_format(style='plain')\n",
    "        plt.title(title)\n",
    "        plt.bar(range(len(target_dict)), values, tick_label=names)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "\n",
    "    def execute(self):\n",
    "        min_date, max_date = self.find_min_n_max(self.df)\n",
    "        payments_dict, count_dict = self.find_stats(self.df, min_date, max_date)\n",
    "        avg_payments_dict = self.calculate_avg_payments(payments_dict, count_dict)\n",
    "        \n",
    "        self.plot(payments_dict, \"Total Payments\")\n",
    "        self.plot(count_dict, \"Number of Payments\")\n",
    "        self.plot(avg_payments_dict, \"Average Payment\")\n",
    "\n",
    "# DateEDA(train_merged).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354d1083",
   "metadata": {},
   "source": [
    "### Back Payment Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a47c2f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Back Payment Features\n",
    "\n",
    "\n",
    "def add_back_feature(df, n=1):\n",
    "    df[f\"b{n}\"] = df.apply(lambda row: back_feature(row[\"SplitPaymentsHistory\"], n), axis=1)\n",
    "    return df\n",
    "\n",
    "train_merged = add_back_feature(train_merged, 1)\n",
    "train_merged = add_back_feature(train_merged, 2)\n",
    "train_merged = add_back_feature(train_merged, 3)\n",
    "train_merged = add_back_feature(train_merged, 4)\n",
    "train_merged = add_back_feature(train_merged, 5)\n",
    "\n",
    "# test_merged = add_back_feature(test_merged, 1)\n",
    "# test_merged = add_back_feature(test_merged, 2)\n",
    "# test_merged = add_back_feature(test_merged, 3)\n",
    "# test_merged = add_back_feature(test_merged, 4)\n",
    "# test_merged = add_back_feature(test_merged, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d714cd7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9336, 37)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b99c33",
   "metadata": {},
   "source": [
    "### Date Related Features\n",
    "\n",
    "TODO: \n",
    "1. Skipped payments and duration of skipping payments\n",
    "2. Check for custom seasonality (Weather changes EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac33807a",
   "metadata": {},
   "source": [
    "### Backward Feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5015704",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged[\"RegistrationDateParsed\"] = pd.to_datetime(train_merged[\"RegistrationDate\"], infer_datetime_format=True)\n",
    "train_merged[\"ExpectedTermDateParsed\"] = pd.to_datetime(train_merged[\"ExpectedTermDate\"], infer_datetime_format=True)\n",
    "train_merged[\"FirstPaymentDateParsed\"] = pd.to_datetime(train_merged[\"FirstPaymentDate\"], infer_datetime_format=True)\n",
    "train_merged[\"LastPaymentDateParsed\"] = pd.to_datetime(train_merged[\"LastPaymentDate\"], infer_datetime_format=True)\n",
    "\n",
    "train_merged[\"RegistrationDate\"] = pd.to_datetime(train_merged[\"RegistrationDate\"], infer_datetime_format=True).dt.date\n",
    "train_merged[\"ExpectedTermDate\"] = pd.to_datetime(train_merged[\"ExpectedTermDate\"], infer_datetime_format=True).dt.date\n",
    "train_merged[\"FirstPaymentDate\"] = pd.to_datetime(train_merged[\"FirstPaymentDate\"], infer_datetime_format=True).dt.date\n",
    "train_merged[\"LastPaymentDate\"] = pd.to_datetime(train_merged[\"LastPaymentDate\"], infer_datetime_format=True).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c386ce1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_merged[\"LastFirstDuration\"] = (train_merged.LastPaymentDate - train_merged.FirstPaymentDate).astype(\"timedelta64[M]\")\n",
    "train_merged[\"ExpectedFirstDuration\"] = (train_merged.ExpectedTermDate - train_merged.FirstPaymentDate).astype(\"timedelta64[M]\")\n",
    "train_merged[\"LastRegistrationDuration\"] = (train_merged.LastPaymentDate - train_merged.RegistrationDate).astype(\"timedelta64[M]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139edcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Months\n",
    "train_merged[\"FirstPaymentMonth\"] = train_merged[\"FirstPaymentDateParsed\"].dt.month\n",
    "train_merged[\"LastPaymentMonth\"] = train_merged[\"LastPaymentDateParsed\"].dt.month\n",
    "train_merged[\"RegistrationMonth\"] = train_merged[\"RegistrationDateParsed\"].dt.month\n",
    "\n",
    "# # Sine\n",
    "# train_merged[\"FirstPaymentMonthSin\"] = np.sin((train_merged.FirstPaymentMonth-1)*(2.*np.pi/12))\n",
    "# train_merged[\"LastPaymentMonthSin\"] = np.sin((train_merged.LastPaymentMonth-1)*(2.*np.pi/12))\n",
    "# train_merged[\"RegistrationMonthSin\"] = np.sin((train_merged.RegistrationMonth-1)*(2.*np.pi/12))\n",
    "\n",
    "# # Cos\n",
    "# train_merged[\"FirstPaymentMonthCos\"] = np.cos((train_merged.FirstPaymentMonth-1)*(2.*np.pi/12))\n",
    "# train_merged[\"LastPaymentMonthCos\"] = np.cos((train_merged.LastPaymentMonth-1)*(2.*np.pi/12))\n",
    "# train_merged[\"RegistrationMonthCos\"] = np.cos((train_merged.RegistrationMonth-1)*(2.*np.pi/12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e516789a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_merged.drop([\"RegistrationDateParsed\", \"ExpectedTermDateParsed\", \"FirstPaymentDateParsed\", \"LastPaymentDateParsed\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee60a466",
   "metadata": {},
   "source": [
    "### We can ignore UpSell since it represents only 2.7% of the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57809d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged.UpsellDate.count() / train_merged.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8194cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged.nb_payments.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e8073d",
   "metadata": {},
   "source": [
    "### Drop irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "789ccc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cols(df):\n",
    "    df.drop(\n",
    "        [\n",
    "#           \"ID\",\n",
    "            \"UpsellDate\",\n",
    "            \"PaymentMethod\",\n",
    "            \"TransactionDates\",\n",
    "            \"PaymentsHistory\",\n",
    "            \"SupplierName\",\n",
    "            \"Town\",\n",
    "            \"RegistrationDateParsed\",\n",
    "            \"ExpectedTermDateParsed\",\n",
    "            \"FirstPaymentDateParsed\",\n",
    "            \"LastPaymentDateParsed\"\n",
    "        ],\n",
    "        inplace=True,\n",
    "        axis=1\n",
    "    )\n",
    "drop_cols(train_merged)\n",
    "# drop_cols(test_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53cdf99b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9336, 31)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05884d0",
   "metadata": {},
   "source": [
    "### Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400d685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE\n",
    "\n",
    "def encode_and_bind(df, feature_name):\n",
    "    dummies = pd.get_dummies(df[[feature_name]])\n",
    "    res = pd.concat([df, dummies], axis=1)\n",
    "    res = res.drop([feature_name], axis=1)\n",
    "    return(res)\n",
    "\n",
    "for feature in [\"MainApplicantGender\", \"Region\", \"Occupation\", \"rateTypeEntity\"]:\n",
    "    train_merged = encode_and_bind(train_merged, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00ceb28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9059e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged.drop([\"RegistrationDate\", \"ExpectedTermDate\", \"FirstPaymentDate\", \"LastPaymentDate\", \"SplitPaymentsHistory\", \"SplitTransactionDates\", \"m2\", \"m3\", \"m4\", \"m5\", \"m6\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb66d10",
   "metadata": {},
   "source": [
    "\n",
    "### Data Leakage here. Will be handled later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d91d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_merged.Age.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1afae1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_merged.Age = train_merged.Age.fillna(train_merged.Age.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2464ae72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(0)\n",
    "def predict_model(train_array, target):\n",
    "    model = xgb.XGBRegressor(use_label_encoder=False,\n",
    "                             verbosity=0,\n",
    "                             objective=\"reg:squarederror\",\n",
    "                             reg_lambda=0,\n",
    "                             reg_alpha=50)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_array, target, test_size=0.20)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    feature_importance = model.feature_importances_\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = pow(mse, 0.5)\n",
    "    \n",
    "    print(\"---------TEST---------\")\n",
    "    print(\"Predictions\")\n",
    "    print(y_pred)\n",
    "    print(\"RMSE:\", rmse)\n",
    "    \n",
    "    y_pred = model.predict(X_train)\n",
    "    feature_importance = model.feature_importances_\n",
    "    mape = mean_absolute_percentage_error(y_train, y_pred)\n",
    "    mse = mean_squared_error(y_train, y_pred)\n",
    "    rmse = pow(mse, 0.5)\n",
    "    \n",
    "    print(\"---------TRAIN--------\")\n",
    "    print(y_pred)\n",
    "    print(\"RMSE:\", rmse)\n",
    "    \n",
    "    return y_pred, mape, mse, feature_importance, rmse\n",
    "\n",
    "predict_model(train_merged.drop([\"m1\"], axis=1), train_merged[\"m1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8199585",
   "metadata": {},
   "source": [
    "### Saving the data to data/train.csv\n",
    "\n",
    "Features created till cell 6 (and cell 9) are selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1520f50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged.drop(columns=['SplitTransactionDates', \n",
    "                           'RegistrationDate', ], inplace=True)\n",
    "# test_merged.drop(columns=['SplitTransactionDates', \n",
    "#                            'RegistrationDate', ], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1997dc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28007, 35)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72406602",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged.to_csv('../../data/processed/train.csv')\n",
    "test_merged.to_csv('../../data/processed/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eef50d",
   "metadata": {},
   "source": [
    "### Outlier Handling\n",
    "### Advance Modelling\n",
    "### EDA for adding time features\n",
    "### Scaling & Normalization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zindi_payg",
   "language": "python",
   "name": "zindi_payg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}